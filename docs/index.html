<!DOCTYPE html>
<html>
    <title>MoPA-RL</title>

    <meta charset="UTF-8">
    <meta property="og:title" content=MoPA-RL>
    <meta property="og:description" content="Jun et al. Motion Planner Augmented Reinforcement Learning .">
    <meta property="og:url" content="">
    <meta property="og:image" content="">
    <meta property="og:type" content="website">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="icon" type="image/png" href="img/favicon-32x32.png">
    <link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">
    <link href="https://fonts.googleapis.com/css?family=Roboto:100, 100i,300,400,500,700,900" rel="stylesheet">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.1/css/all.css" integrity="sha384-50oBUHEmvpQ+1lW4y57PTFmhCaXp0ML5d60M1M7uH2+nqUivzIebhndOJK28anvf" crossorigin="anonymous">

    <!-- Showdown -->
    <script src=" https://cdnjs.cloudflare.com/ajax/libs/showdown/1.9.0/showdown.min.js"></script>
    <script src="js/figure-extension.js"></script>

    <!-- jQuery -->
    <script src="https://code.jquery.com/jquery-3.3.1.min.js" integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin="anonymous"></script>

    <!-- WAVE -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/materialize/1.0.0/css/materialize.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/materialize/1.0.0/js/materialize.min.js"></script>

    <!-- Slick -->
    <link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/slick-carousel@1.8.1/slick/slick.css"/>
    <link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/slick-carousel@1.8.1/slick/slick-theme.css"/>
    <script type="text/javascript" src="//cdn.jsdelivr.net/npm/slick-carousel@1.8.1/slick/slick.min.js"></script>

    <link rel="stylesheet" href="theme.css">

    <script>
        const classMap = {
            ul: 'browser-default'
        }

        const bindings = Object.keys(classMap)
        .map(key => ({
            type: 'output',
            regex: new RegExp(`<${key}(.*)>`, 'g'),
            replace: `<${key} class="${classMap[key]}" $1>`
        }));

        const converter = new showdown.Converter({
            extensions: [bindings, 'figure']
        });
        converter.setOption('parseImgDimensions', true);
        converter.setOption('tables', true);
        converter.setFlavor('github');

        $("#markdown-body").ready(() => {
            $.get( "content.md", (data) => {
                const content_html = converter.makeHtml(data);
                $("#markdown-body").html(content_html);
            });
        })

        $(document).ready(function() {
            $('.slideshow').slick({
                slidesToShow: 1,
                slidesToScroll: 1,
                autoplay: true,
                autoplaySpeed: 2000,
                dots: true,
                arrows: false,
                infinite: true,
                fade: true,
                speed: 300,
                cssEase: 'linear'
            });
        });

    </script>

    <body>
        <!-- Header -->
        <!-- Wide screen -->
        <header class="hd-container w3-container hide-narrow content-center">
            <div class="w3-cell-row" style="width: 90%; margin: auto; max-width: 1600px; margin-top: 80px; margin-bottom: 40px">
                <div class="w3-container w3-cell w3-cell-middle">
                    <div class="title">Motion Planner Augmented Reinforcement Learning</div>
                    <div class="title">for Robot Manipulation in obstructed Environments</div>
                    <!-- Author -->
                <div class="w3-row-padding">
                    <div class="authorship-container">
                        <ul class="horizontal-list">
                            <li><a href="https://junjungoal.tech" target="_blank"><i class="far fa-user"></i> Jun Yamada<sup>1</sup></a></li>
                            <li><a href="https://youngwoon.github.io" target="_blank"><i class="far fa-user"></i> Youngwoon Lee<sup>1</sup></a></li>
                            <li><a href="https://www.gautamsalhotra.com/" target="_blank"><i class="far fa-user"></i> Gautam Salhotra<sup>2</sup> </a></li>
                            <li><a href="https://kpertsch.github.io" target="_blank"><i class="far fa-user"></i> Karl Pertsch<sup>1</sup> </a></li>
                            <li><a href="https://mpflueger.github.io" target="_blank"><i class="far fa-user"></i> Max Pflueger<sup>2</sup> </a></li>
                            <li><a href="http://robotics.usc.edu/~gaurav" target="_blank"><i class="far fa-user"></i> Gaurav S. Sukhatme<sup>2</sup> </a></li>
                            <li><a href="https://viterbi-web.usc.edu/~limjj/" target="_blank"><i class="far fa-user"></i> Joseph J. Lim<sup>1</sup> </a></li>
                            <li><a href="http://www.peter-englert.net/" target="_blank"><i class="far fa-user"></i> Peter Englert<sup>2</sup> </a></li>
                        </ul>
                        <span class="school"><a href="https://clvrai.com/" target="_blank"><i class="fas fa-university"></i> Cognitive Learning for Vision and Robotics (CLVR), USC<sup>1</sup> </a></span>
                        <span class="school"><a href="https://clvrai.com/" target="_blank"><i class="fas fa-university"></i> Robotics Embedded Systems Laboratory (RESL), USC<sup>2</sup> </a></span>
                    </div>
                    <div class="w3-card-4 w3-round-large furniture-grid" style="width: 80%; max-width: 700px">
                            <img width="100%" height="100%" src="img/teaser.png">
                    </div>

                    </div>
                    <div class="excerpt w3-padding-16" style="width: 80%; max-width: 700px; margin: auto;">
                        Deep reinforcement learning (RL) agents are able to learn contact-rich manipulation tasks by maximizing a reward signal, but require large amounts of experience, especially in environments with many obstacles that complicate exploration. In contrast, motion planners use explicit models of the agent and environment to plan collision-free paths to faraway goals, but suffer from inaccurate models in tasks that require contacts with the environment. To combine the benefits of both approaches, we propose motion planner augmented RL (MoPA-RL) which augments the action space of an RL agent with the long-horizon planning capabilities of motion planners. Based on the magnitude of the action, our approach smoothly transitions between directly executing the action and invoking a motion planner. We demonstrate that MoPA-RL increases learning efficiency, leads to a faster exploration of the environment, and results in safer policies that avoid collisions with the environment.
                    </div>
                </div>
            </div>
        </header>

        <!-- Narrow screen -->
        <header class="hd-container w3-container hide-wide">
            <div class="w3-row-padding w3-center w3-padding-24">
                <span class="title">Motion Planner Augmented Reinforcement Learning <br/> for Robot Manipulation in Obstructed Environments</span>
            </div>
            <div class="w3-row-padding">
                <!-- Author -->
                <div class="authorship-container">
                    <ul class="horizontal-list">
                        <li><a href="https://junjungoal.tech" target="_blank"><i class="far fa-user"></i> Jun Yamada</a></li>
                        <li><a href="https://youngwoon.github.io" target="_blank"><i class="far fa-user"></i> Youngwoon Lee</a></li>
                        <li><a href="https://www.gautamsalhotra.com/" target="_blank"><i class="far fa-user"></i> Gautam Salhotra</a></li>
                        <li><a href="https://kpertsch.github.io" target="_blank"><i class="far fa-user"></i> Karl Pertsch</a></li>
                        <li><a href="https://mpflueger.github.io" target="_blank"><i class="far fa-user"></i> Max Pflueger</a></li>
                        <li><a href="http://robotics.usc.edu/~gaurav" target="_blank"><i class="far fa-user"></i> Gaurav S. Sukhatme</a></li>
                        <li><a href="https://viterbi-web.usc.edu/~limjj/" target="_blank"><i class="far fa-user"></i> Joseph J. Lim</a></li>
                        <li><a href="http://www.peter-englert.net/" target="_blank"><i class="far fa-user"></i> Peter Englert</a></li>
                    </ul>
                    <span class="school"><a href="https://clvrai.com/" target="_blank"><i class="fas fa-university"></i> Cognitive Learning for Vision and Robotics (CLVR), USC</a></span>
                </div>

            </div>
            <div class="w3-row-padding w3-center w3-padding-16">
                <div class="w3-card-4 w3-round-large furniture-grid" style="width: 80%; max-width: 400px">
                        <img width="100%" height="100%" src="img/teaser.png">
                </div>
            </div>
            <div class="w3-row-padding"><hr></div>
            <div class="w3-row-padding w3-padding-16">
                <div class="excerpt">
                        Deep reinforcement learning (RL) agents are able to learn contact-rich manipulation tasks by maximizing a reward signal, but require large amounts of experience, especially in environments with many obstacles that complicate exploration. In contrast, motion planners use explicit models of the agent and environment to plan collision-free paths to faraway goals, but suffer from inaccurate models in tasks that require contacts with the environment. To combine the benefits of both approaches, we propose motion planner augmented RL (MoPA-RL) which augments the action space of an RL agent with the long-horizon planning capabilities of motion planners. Based on the magnitude of the action, our approach smoothly transitions between directly executing the action and invoking a motion planner. We demonstrate that MoPA-RL increases learning efficiency, leads to a faster exploration of the environment, and results in safer policies that avoid collisions with the environment.
                </div>
            </div>
        </header>

        <!-- Main Body -->
        <div class="main-body">
            <div class="w3-container">
                <div class="w3-content" style="max-width:1000px">

                    <!-- Links -->
                    <div class="link-container">
                        <ul class="horizontal-list">
                            <li><button class="w3-button waves-effect waves-light w3-card-4 grey lighten-2 w3-round-large"><i class="fas fa-file-alt"></i> <a href="https://openreview.net/forum?id=ryxB2lBtvH" target="_blank"> Paper </a></button></li>
                            <li><button class="w3-button waves-effect waves-light w3-card-4 grey lighten-2 w3-round-large"><i class="fas fa-code"></i> <a href="https://github.com/clvrai/coordination" target="_blank"> Code </a></button></li>
                            <!-- <li><button class="w3&#45;button waves&#45;effect waves&#45;light w3&#45;card&#45;4 grey lighten&#45;2 w3&#45;round&#45;large"><i class="fas fa&#45;file&#45;powerpoint"></i> <a href="https://clvrai.com/assets/ICLR2020_5mins.pdf" target="_blank"> Slide </a></button></li> -->
                        </ul>
                    </div>

                    <!-- <!&#45;&#45; Motivation &#45;&#45;> -->
                    <!-- <div class="slideshow_wrapper"> -->
                    <!--     <h2>Motivation</h2> -->
                    <!--     <p style="text&#45;align:justify"> -->
                    <!--       When mastering a complex manipulation task, humans often decompose the task into sub&#45;skills of their body parts, practice the sub&#45;skills independently, and then execute the sub&#45;skills together. Similarly, a robot with multiple end&#45;effectors can efficiently learn to perform complex tasks by coordinating sub&#45;skills of each end&#45;effector. -->
                    <!--     </p> -->
                    <!--     <figure> -->
                    <!--         <img src="./img/website_motivation_figure2.png" alt="Motivation illustration" title="Motivation illustration"> -->
                    <!--         <figcaption> Motivation illustration </figcaption> -->
                    <!--     </figure> -->
                    <!-- </div> -->
                    <!--  -->
                    <!-- <hr> -->
                    <!--  -->
                    <!-- <!&#45;&#45; Slideshow &#45;&#45;> -->
                    <!-- <div class="slideshow_wrapper"> -->
                    <!--     <h2>Problem</h2> -->
                    <!--     <div class="slideshow"> -->
                    <!--         <div><img src="./img/teaser_0.png" /></div> -->
                    <!--         <div><img src="./img/teaser_1.png" /></div> -->
                    <!--         <div><img src="./img/teaser_2.png" /></div> -->
                    <!--         <div><img src="./img/teaser_3.png" /></div> -->
                    <!--         <div><img src="./img/teaser_4.png" /></div> -->
                    <!--         <div><img src="./img/teaser_5.png" /></div> -->
                    <!--         <div><img src="./img/teaser_6.png" /></div> -->
                    <!--         <div><img src="./img/teaser_7.png" /></div> -->
                    <!--     </div> -->
                    <!-- </div> -->

                    <!-- Markdown Body -->
                    <div id="markdown-body"></div>
                </div>
            </div>
        </div>

        <!-- Footer -->
        <footer class="w3-center w3-light-grey w3-padding-32 w3-small">
            <p style="color: grey">The authors would like to thank Karl Pertsch and many members of the USC CLVR lab and USC RESL for helpful discussion.</br> &copy; Copyright 2020, CLVR, USC.</p>
        </footer>

    </body>
</html>
