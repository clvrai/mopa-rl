<!DOCTYPE html>
<html>
    <title>MoPA-RL</title>

    <meta charset="UTF-8">
    <meta property="og:title" content=MoPA-RL>
    <meta property="og:description" content="Yamada et al. Motion Planner Augmented Reinforcement Learning for Robot Manipulation in Obstructed Environments">
    <meta property="og:url" content="">
    <meta property="og:image" content="">
    <meta property="og:type" content="website">
    <meta name="viewport" content="width=device-width, initial-scale=1 minimum-scale=1.0">

    <link rel="icon" type="image/png" href="img/favicon-32x32.png">
    <link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">
    <link href="https://fonts.googleapis.com/css?family=Roboto:100, 100i,300,400,500,700,900" rel="stylesheet">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.1/css/all.css" integrity="sha384-50oBUHEmvpQ+1lW4y57PTFmhCaXp0ML5d60M1M7uH2+nqUivzIebhndOJK28anvf" crossorigin="anonymous">

    <!-- Showdown -->
    <script src=" https://cdnjs.cloudflare.com/ajax/libs/showdown/1.9.0/showdown.min.js"></script>
    <script src="js/figure-extension.js"></script>

    <!-- jQuery -->
    <script src="https://code.jquery.com/jquery-3.3.1.min.js" integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin="anonymous"></script>

    <!-- WAVE -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/materialize/1.0.0/css/materialize.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/materialize/1.0.0/js/materialize.min.js"></script>

    <!-- Slick -->
    <link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/slick-carousel@1.8.1/slick/slick.css"/>
    <link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/slick-carousel@1.8.1/slick/slick-theme.css"/>
    <script type="text/javascript" src="//cdn.jsdelivr.net/npm/slick-carousel@1.8.1/slick/slick.min.js"></script>

    <link rel="stylesheet" href="theme.css">

    <script>
        const classMap = {
            ul: 'browser-default'
        }

        const bindings = Object.keys(classMap)
        .map(key => ({
            type: 'output',
            regex: new RegExp(`<${key}(.*)>`, 'g'),
            replace: `<${key} class="${classMap[key]}" $1>`
        }));

        const converter = new showdown.Converter({
            extensions: [bindings, 'figure']
        });
        converter.setOption('parseImgDimensions', true);
        converter.setOption('tables', true);
        converter.setFlavor('github');

        $("#markdown-body").ready(() => {
            $.get( "content.md", (data) => {
                const content_html = converter.makeHtml(data);
                $("#markdown-body").html(content_html);
            });
        });

    </script>

    <body>
        <!-- Header -->
        <!-- Wide screen -->
        <header class="hd-container w3-container hide-narrow content-center">
            <div class="w3-cell-row" style="width: 90%; margin: auto; max-width: 1600px; margin-top: 80px; margin-bottom: 40px">
                <div class="w3-container w3-cell w3-cell-middle">
                    <div class="title">Motion Planner Augmented Reinforcement Learning</div>
                    <div class="title">for Robot Manipulation in Obstructed Environments</div>
                    <!-- Author -->
                <div class="w3-row-padding">
                    <div class="authorship-container">
                        <ul class="horizontal-list">
                            <li><a href="https://junjungoal.github.io/" target="_blank"><i class="far fa-user"></i> Jun Yamada<sup>* 1</sup></a></li>
                            <li><a href="https://youngwoon.github.io" target="_blank"><i class="far fa-user"></i> Youngwoon Lee<sup>* 1</sup></a></li>
                            <li><a href="https://www.gautamsalhotra.com/" target="_blank"><i class="far fa-user"></i> Gautam Salhotra<sup>2</sup> </a></li>
                            <li><a href="https://kpertsch.github.io" target="_blank"><i class="far fa-user"></i> Karl Pertsch<sup>1</sup> </a></li>
                            <li><a href="https://mpflueger.github.io" target="_blank"><i class="far fa-user"></i> Max Pflueger<sup>2</sup> </a></li>
                            <li><a href="http://robotics.usc.edu/~gaurav" target="_blank"><i class="far fa-user"></i> Gaurav S. Sukhatme<sup>2</sup> </a></li>
                            <li><a href="https://viterbi-web.usc.edu/~limjj/" target="_blank"><i class="far fa-user"></i> Joseph J. Lim<sup>1</sup> </a></li>
                            <li><a href="http://www.peter-englert.net/" target="_blank"><i class="far fa-user"></i> Peter Englert<sup>2</sup> </a></li>
                        </ul>
                        <span class="school"><a href="https://clvrai.com/" target="_blank"><i class="fas fa-university"></i> Cognitive Learning for Vision and Robotics (CLVR), USC<sup>1</sup> </a></span>
                        <span class="school"><a href="https://robotics.usc.edu/resl/" target="_blank"><i class="fas fa-university"></i> Robotics Embedded Systems Laboratory (RESL), USC<sup>2</sup> </a></span>
                    </div>
                    <div class="w3-card-4 w3-round-large furniture-grid" style="width: 80%; max-width: 700px">
                            <!-- <img width="100%" height="100%" src="img/teaser.png"> -->
                            <img width="100%" height="100%" src="video/teaser.gif">
                    </div>

                    </div>
                    <div class="excerpt w3-padding-16" style="width: 80%; max-width: 700px; margin: auto;">
                        Deep reinforcement learning (RL) agents are able to learn contact-rich manipulation tasks by maximizing a reward signal, but require large amounts of experience, especially in environments with many obstacles that complicate exploration. In contrast, motion planners use explicit models of the agent and environment to plan collision-free paths to faraway goals, but suffer from inaccurate models in tasks that require contacts with the environment. To combine the benefits of both approaches, we propose motion planner augmented RL (MoPA-RL) which augments the action space of an RL agent with the long-horizon planning capabilities of motion planners. Based on the magnitude of the action, our approach smoothly transitions between directly executing the action and invoking a motion planner. We demonstrate that MoPA-RL increases learning efficiency, leads to a faster exploration of the environment, and results in safer policies that avoid collisions with the environment.
                    </div>
                </div>
            </div>
        </header>

        <!-- Narrow screen -->
        <header class="hd-container w3-container hide-wide">
            <div class="w3-row-padding w3-center w3-padding-24">
                <span class="title">Motion Planner Augmented Reinforcement Learning <br/> for Robot Manipulation in Obstructed Environments</span>
            </div>
            <div class="w3-row-padding">
                <!-- Author -->
                <div class="authorship-container">
                    <ul class="horizontal-list">
                        <li><a href="https://junjungoal.tech" target="_blank"><i class="far fa-user"></i> Jun Yamada*</a></li>
                        <li><a href="https://youngwoon.github.io" target="_blank"><i class="far fa-user"></i> Youngwoon Lee*</a></li>
                        <li><a href="https://www.gautamsalhotra.com/" target="_blank"><i class="far fa-user"></i> Gautam Salhotra</a></li>
                        <li><a href="https://kpertsch.github.io" target="_blank"><i class="far fa-user"></i> Karl Pertsch</a></li>
                        <li><a href="https://mpflueger.github.io" target="_blank"><i class="far fa-user"></i> Max Pflueger</a></li>
                        <li><a href="http://robotics.usc.edu/~gaurav" target="_blank"><i class="far fa-user"></i> Gaurav S. Sukhatme</a></li>
                        <li><a href="https://viterbi-web.usc.edu/~limjj/" target="_blank"><i class="far fa-user"></i> Joseph J. Lim</a></li>
                        <li><a href="http://www.peter-englert.net/" target="_blank"><i class="far fa-user"></i> Peter Englert</a></li>
                    </ul>
                    <span class="school"><a href="https://clvrai.com/" target="_blank"><i class="fas fa-university"></i> Cognitive Learning for Vision and Robotics (CLVR), USC</a></span> <br/>
                    <span class="school"><a href="https://robotics.usc.edu/resl/" target="_blank"><i class="fas fa-university"></i> Robotics Embedded Systems Laboratory (RESL), USC</a></span>
                </div>

            </div>
            <div class="w3-row-padding w3-center w3-padding-16">
                <div class="w3-card-4 w3-round-large furniture-grid" style="width: 80%; max-width: 400px">
                        <img width="100%" height="100%" src="img/teaser.png">
                </div>
            </div>
            <div class="w3-row-padding"><hr></div>
            <div class="w3-row-padding w3-padding-16">
                <div class="excerpt">
                        Deep reinforcement learning (RL) agents are able to learn contact-rich manipulation tasks by maximizing a reward signal, but require large amounts of experience, especially in environments with many obstacles that complicate exploration. In contrast, motion planners use explicit models of the agent and environment to plan collision-free paths to faraway goals, but suffer from inaccurate models in tasks that require contacts with the environment. To combine the benefits of both approaches, we propose motion planner augmented RL (MoPA-RL) which augments the action space of an RL agent with the long-horizon planning capabilities of motion planners. Based on the magnitude of the action, our approach smoothly transitions between directly executing the action and invoking a motion planner. We demonstrate that MoPA-RL increases learning efficiency, leads to a faster exploration of the environment, and results in safer policies that avoid collisions with the environment.
                </div>
            </div>
        </header>

        <!-- Main Body -->
        <div class="main-body">
            <div class="w3-container">
                <div class="w3-content" style="max-width:1000px;">
                    <!-- Links -->
                    <div class="link-container">
                        <ul class="horizontal-list">
                            <li><button class="w3-button waves-effect waves-light w3-card-4 grey lighten-2 w3-round-large"><i class="fas fa-file-alt"></i> <a href="https://arxiv.org/abs/2010.11940" target="_blank"> Paper </a></button></li>
                            <li><button class="w3-button waves-effect waves-light w3-card-4 grey lighten-2 w3-round-large"><i class="fas fa-code"></i> <a href="https://github.com/clvrai/mopa-rl" target="_blank"> Code </a></button></li>
                        </ul>
                    </div>
                    <!-- Markdown Body -->
                    <div id="markdown-body"></div>
                </div>
            </div>
        </div>

        <!-- Footer -->
        <footer class="w3-center w3-light-grey w3-padding-32 w3-small">
            <p style="color: grey">The authors would like to thank many members of the CLVR lab and RESL for helpful discussion.</br> &copy; Copyright 2020, CLVR, USC.</p>
        </footer>

    </body>
</html>
